from langchain_core.prompts import PromptTemplate


def get_base_ddl_prompt():
    return """You are a Snowflake SQL schema generation expert.

Your task is to generate clean, minimal, and Snowflake-compatible base DDL statements from a JSON schema.

## Instructions:
- Generate one CREATE OR REPLACE TABLE per table in the schema.
- **Do NOT include schema prefixes (e.g., use `D_COUNTRY` instead of `ODS_HR.D_COUNTRY`).**
- The surrogate key named SK_<table_name> defined as:
  SK_<table_name> NUMBER AUTOINCREMENT PRIMARY KEY
- Define only the columns and the contraints present in the JSON schema.
- Do NOT include constraints other than the surrogate primary key.
- Use Snowflake-compatible data types (e.g., NUMBER, VARCHAR, BOOLEAN, TIMESTAMP).
- Use UPPERCASE for SQL keywords and snake_case for identifiers.
- Match names exactly from the JSON schema.

## Output Format:
- Include -- SECTION: <TABLE NAME> before each table definition
- One CREATE TABLE statement per table
- No constraints (except surrogate PK), clustering keys, or comments
- No extra formatting, explanations, or metadata

Now generate the base DDL.
"""


def get_main_agent_prompt():
    return """You are an intelligent assistant responsible for generating and refining a Snowflake-compatible DDL script based on a structured JSON schema.

## Objective:
Your goal is to create a complete and optimized DDL script for Snowflake from JSON metadata using available tools.

## Workflow Steps:
1. Generate base tables with primary key (surrogate key only).
2. Add constraints (UNIQUE, NOT NULL, FOREIGN KEY, CHECK) via ALTER TABLE.
3. Suggest clustering keys (CLUSTER BY).
4. Add documentation (COMMENT ON statements).
5. Validate the final DDL syntax.

## Rules:
- Follow the ReAct format: Reasoning + Action + Observation.
- Use tools in sequence. Do not skip or combine steps.
- Always maintain this output structure in the final DDL:
    a. All CREATE TABLE statements first Only  
    b. Then all ALTER TABLE statements for constraints  
    c. Then any clustering or indexes  
    d. Then COMMENT ON statements

## Naming:
- Use SK_<table_name> NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY for surrogate keys.
- Use UPPERCASE for SQL keywords, snake_case for identifiers.

## RESPONSE FORMAT:
Thought: What step should I take next?
Action: The action to take (must be one of: {tool_names})
Action Input: JSON-formatted input for the selected action
Observation: Result from executing the action
... (repeat as needed)
Thought: I have completed all steps and have the final DDL
Final Answer: FINAL_DDL:
[Paste the complete DDL here with CREATE first, then ALTER, then CLUSTER, then COMMENTS only. No other text.]

Begin!
"""


def get_validation_prompt():
    return """You are a database expert specializing in Snowflake SQL.

Your task is to validate the following DDL for syntax correctness.

Instructions:
- Check the SQL syntax according to Snowflake's standards.
- Identify and explain any syntax issues clearly.
- If the syntax is correct, respond with: "Valid Snowflake DDL."

DDL to validate:
{ddl}
"""


def get_constraints_prompt():
    return """You are a Snowflake SQL architect.

Your task is to analyze the provided CREATE TABLE statements and generate all missing constraints using ALTER TABLE statements, in accordance with best practices and Snowflake limitations.

## Constraint Rules:

1. **PRIMARY KEY Constraints**
   - Add only if missing in the CREATE TABLE.
   - Only apply to surrogate keys (`SK_` prefix).
   - Use the format: `PRIMARY KEY (column)` without naming the constraint.

2. **UNIQUE Constraints**
   - Add for all business keys (`_ID` suffix) that are not surrogate keys (`SK_` prefix).
   - Use the format: `ALTER TABLE <table> ADD CONSTRAINT UK_<table>_<column> UNIQUE (<column>);`

3. **NOT NULL Constraints**
   - Add to all business-critical columns, especially:
     - Business key columns (`_ID`)
     - Name fields (columns with `NAME`)
     - Date fields (columns with `DATE` or `DT_`)
   - Do **not** apply NOT NULL to columns already covered by PRIMARY KEY or explicitly marked as NOT NULL.
   - Use the format: `ALTER TABLE <table> MODIFY COLUMN <column> SET NOT NULL;`

4. **FOREIGN KEY Constraints**
   - Add for all columns with names starting with `SK_` that likely reference surrogate keys in other tables.
   - Use the format: `ALTER TABLE <table> ADD CONSTRAINT FK_<table>_<ref_table> FOREIGN KEY (<column>) REFERENCES <ref_table>(<ref_column>);`
   - Use a best-guess for `<ref_table>` based on the column name (e.g., `SK_REGION_ID` → `REGION.SK_REGION_ID`)

5. **CHECK Constraints**
   - Add only for known business rules (e.g., value ranges or enums), if explicitly provided in the schema or column comments.

## Implementation Rules:
- Use **only ALTER TABLE** statements.
- **Do not modify the CREATE TABLE definitions.**
- Preserve all existing constraints in the DDL.
- Avoid redundancy:
  - Do not add UNIQUE or NOT NULL to PRIMARY KEY columns.
  - Do not duplicate existing NOT NULL or UNIQUE constraints.
  - Avoid naming constraints for PRIMARY KEY (Snowflake auto-generates these).

## Output Requirements:
- Return only valid SQL statements.
- Keep the original CREATE TABLE block unchanged.
- Append all ALTER TABLE statements after the CREATE TABLE blocks.
- Do not return any explanation, description, or formatting outside of SQL.

Input Schema:
{schema}

Original DDL:
{ddl}

Generate all required ALTER TABLE statements to enforce the constraints.
"""


def get_clustering_prompt():
    return """You are a Snowflake performance tuning expert.

Your task is to enhance the provided DDL with CLUSTER BY clauses based on data distribution and query access patterns.

## Instructions:
- Suggest CLUSTER BY only on columns with high cardinality or frequent filtering.
- Add the CLUSTER BY clause at the end of the relevant CREATE TABLE statements.
- Do not modify column definitions or existing constraints.
- Do not include any explanation or notes.

Schema:
{schema}

Original DDL:
{ddl}

Return only the updated DDL with CLUSTER BY clauses added.
"""


def get_comments_prompt():
    return """You are a Snowflake data engineer.

Your task is to document the schema using COMMENT statements for tables and columns.

## Instructions:
- Use COMMENT ON TABLE and COMMENT ON COLUMN syntax.
- Skips NULL comments (Snowflake doesn't support them).
- Derive comments from schema field names, types, and metadata where available.
- Do NOT alter CREATE TABLE or ALTER TABLE definitions.
- Place COMMENT statements after all CREATE and ALTER TABLE statements.

Schema:
{schema}

Original DDL:
{ddl}

Output COMMENT statements that enhance the schema documentation.
"""


mapping_prompt = PromptTemplate(
    input_variables=["source_schema", "target_schema"],
    template="""
As a data mapping expert, create precise mapping instructions between these schemas:

SOURCE SCHEMA:
{source_schema}

TARGET SNOWFLAKE SCHEMA:
{target_schema}

Mapping Rules:
1. For dimension tables:
   - the SK_ keys are auto-generated so we don't need it for mapping rule
   - Map natural keys and business attributes

2. For date dimensions:
   - Extract dates from source date fields (don't generate)
   - Example: HIRE_DATE → DATE_VALUE with derived day/month/year

3. For fact tables:
   - Map all foreign keys to dimension SKs
   - Preserve all measures

Output Format per Target Table:
## TRANSFORMATION FOR [Target Table]
1. Target Table: [name]
2. Source Tables: [sources]
3. Natural Keys: [join keys]
4. Column Mapping:
   - [Target] ← [Source] ([Transform])
   - [Derived] ← [Logic]
5. Special Notes:
   - [Data Quality]
   - [Constraints ] For both Foreign key and Unique
   - [Date Handling]

Key Requirements:
- Never use hardcoded dates
- For date dimensions: map from existing date fields
- Skip DT_INSERT field
- Include all necessary joins
- ! NOT use History table like Job_History
- Returned sorted mapping by importance (based on dependency). 
##  RESPONSE
""",
)

SparkPromptDRAFT = PromptTemplate(
    input_variables=["table_name", "table_mapping"],
    template="""
    ## ROLE
    You are an expert data engineer specializing in Spark transformations for Snowflake. Generate production-grade PySpark code that strictly implements the provided mapping specification.

    ## MAPPING SPECIFICATION
    {table_mapping}

    ## REQUIREMENTS
    1. CODE STRUCTURE:
    - Create a function named: transform_{table_name}(source_df, dimension_dfs)
    - source_df: Main source DataFrame containing all source columns
    - dimension_dfs: Dictionary of dimension DataFrames (keys: dimension table names)

    2. TRANSFORMATION RULES:
    - Map columns exactly as specified in the mapping
    - Apply explicit column selection (no SELECT *)
    - Use direct column references (F.col("column_name")) not table-qualified references
    - Implement all specified data quality checks
    - Handle joins with dimension tables when needed
    - Include proper type casting
    - For Fact table:
      - Same process select required column and then FK RESOLUTION to get surrogate keys (SK_ columns) for Dimensional table

    3. OUTPUT REQUIREMENTS:
    - Return a DataFrame ready for Snowflake loading
    - All columns must match target schema exactly
    - Include DT_INSERT with current_timestamp()
    - Exclude surrogate keys (SK_ columns) as they're auto-generated

    4. OPTIMIZATION:
    - Use efficient Spark operations
    - Minimize data shuffling
    - Partition large operations appropriately

    ## EXAMPLE TRANSFORMATION (Generic Structure)
    ```python
    from pyspark.sql import functions as F
    from pyspark.sql.types import *

    def transform_example_table(source_df, dimension_dfs):
        "\"\"\""
        Transforms source data for EXAMPLE_TABLE according to mapping specs
        Args:
            source_df: DataFrame containing source data
            dimension_dfs: Dict of dimension DataFrames {{'DIM1': df1, 'DIM2': df2}}
        Returns:
            Transformed DataFrame matching target schema
        \"\"\"
        
        # 1. MAPPING DES COLONNES
        transformed_df = source_df.select(
            F.col("COL1").alias("CIBLE1"),
            F.col("COL2").alias("CIBLE2")
            ...
        )
        
        # 2. CONVERSION DES TYPES
        transformed_df = transformed_df.withColumn(
            "CIBLE1", F.col("CIBLE1").cast([Type]())
        )
        # 3. CONTROLES QUALITE (si requis)
        if transformed_df.filter(F.col("CIBLE1").isNull()).count() > 0:
            raise ValueError("Valeurs nulles dans CIBLE1")
            
        # 4. DIMENSION JOINS (if specified in mapping)
        dim_table_df = dimension_dfs["D_TABLE"]
        valid_df = (
            valid_df.join(
                dim_table_df.select("NATURAL_KEY", "SK_KEY"),
                valid_df["FK_COL"] == dim_table_df["NATURAL_KEY"],
                "left",
            )
            .withColumn("SK_KEY", F.coalesce(F.col("SK_KEY"), F.lit(-1)))
            .drop("FK_COL")
        )
        
        # 5. SCHEMA DE SORTIE
        valid_output_schema = StructType(
            [
                StructField("CIBLE1", IntegerType(), False),
                StructField("CIBLE2", StringType(), True),
                ...
            ]
        )
        
        # 6. SELECTION FINALE
        final_columns = [
            "CIBLE1",
            "CIBLE2",
            ...
            F.current_timestamp().alias("DT_INSERT"),
        ]
        valid_df = transformed_df.select(final_columns)
        
        return valid_df
    ```
    Generate only the PySpark implementation code with no additional commentary or explanation.
    ## RESPONSE
    """,
)

SparkPrompt = PromptTemplate(
    input_variables=["table_name", "table_mapping"],
    template="""
    ## ROLE
    You are an expert data engineer specializing in Spark transformations for Snowflake. Generate production-grade PySpark code that strictly implements the provided mapping specification.

    ## MAPPING SPECIFICATION
    {table_mapping}

    ## REQUIREMENTS
    1. CODE STRUCTURE:
    - Create a function named: transform_{table_name}(source_df, dimension_dfs)
    - source_df: Main source DataFrame containing all source columns
    - dimension_dfs: Dictionary of dimension DataFrames (keys: dimension table names)
    
    2. TRANSFORMATION RULES:
     - Be sure to select ALL needed columns including:
      * All target columns specified in mapping
      * All natural keys required for dimension joins (even if not in final output)
    - Map columns exactly as specified in the mapping
    - Apply explicit column selection (no SELECT *)
    - Use direct column references (F.col("column_name")) not table-qualified references
    - Implement all specified data quality checks
    - Handle joins with dimension tables when needed only respect MAPPING SPECIFICATION
    - Include proper type casting **when the column target type is not String**
    - For Fact table:
      - Same process select required column and then FK RESOLUTION to get surrogate keys (SK_ columns) for Dimensional table
    - For Date Dimensional table :
      * Use F.to_date() with explicit format when converting from string
      * For date components extraction:
        - Day: F.dayofmonth("DATE_COL").alias("DAY")
        - Month: F.month("DATE_COL").alias("MONTH")
        - Year: F.year("DATE_COL").alias("YEAR")
    ### KEY CONSIDERATION
        1. MANAGEMENT OF NULLS:
        - Business columns (business keys): reject rows with clear error
        - Numeric values: replace NULL with 0 (or average if specified)
        - Strings: replace NULL with 'N/A'
        - Dates: replace NULL with minimum date (1900-01-01) with transformed_df = transformed_df.na.fill("1900/01/01", subset=["DATE_COL"]
    )

        2. AUTOMATIC CASTING:
        - Adapt with snowflake target column type (DecimalType(X, 2) , Integertype , Datetype ...)
        - Analyze the source vs target type
        - String Integer/Double: strict cast
        - String Date: use to_date with specific format
        - Risky conversions: add try_cast with default value
        
    3. OUTPUT REQUIREMENTS:
    - Return a DataFrame ready for Snowflake loading
    - All columns must match target schema exactly
    - Include DT_INSERT with current_timestamp()
    - Exclude surrogate keys (SK_ columns) as they're auto-generated

    4. OPTIMIZATION:
    - Use efficient Spark operations
    - Minimize data shuffling
    - Partition large operations appropriately
    
    5. DIMENSION JOIN POLICY:
    - Perform dimension joins ONLY when:
      * The mapping explicitly requires surrogate key resolution
      * The target column name starts with "SK_"
    - For natural key references (like MANAGER_ID), keep the original Extracted Source value without join .

    ## OUTPUT STRICT STRUCTURE
    ```python
    from pyspark.sql import functions as F
    from pyspark.sql.types import *

    def transform_example_table(source_df, dimension_dfs):
        "\"\"\""
        Transforms source data for EXAMPLE_TABLE according to mapping specs
        Args:
            source_df: DataFrame containing source data
            dimension_dfs: Dict of dimension DataFrames {{'DIM1': df1, 'DIM2': df2}}
        Returns:
            Transformed DataFrame matching target schema
        \"\"\"
        
        # 1. MAPPING DES COLONNES
        transformed_df = source_df.select(
            F.col("COL1").alias("CIBLE1"),
            F.col("COL2").alias("CIBLE2")
            ...
        )
        
        # 2. CONVERSION DES TYPES
        transformed_df = transformed_df.withColumn(
            "CIBLE1", F.col("CIBLE1").cast([Type]())
        )
        # 3. NULL HANDLING POLICY (if required)
        transformed_df = transformed_df.na.fill(.....)
        
        # 4. CONTROLES QUALITE (if required)
        if transformed_df.filter(F.col("CIBLE1").isNull()).count() > 0:
            raise ValueError("Valeurs nulles dans CIBLE1")
            
        # 5. DIMENSION JOINS (if specified in mapping)
        dim_table_df = dimension_dfs["D_TABLE"]
        transformed_df = (
            transformed_df.join(
                dim_table_df.select("NATURAL_KEY", "SK_KEY"),
                valid_df["FK_COL"] == dim_table_df["NATURAL_KEY"],
                "left",
            )
            .withColumn("SK_KEY", F.coalesce(F.col("SK_KEY"), F.lit(-1)))
            .drop("FK_COL")
        )
        # 5. Ajout des métadonnées techniques
        valid_df  = transformed_df.withColumn(
            "DT_INSERT", F.current_timestamp()
        )

        
        # 6. SELECTION FINALE
        final_columns = [
            "CIBLE1",
            "CIBLE2",
            ...
            "DT_INSERT",
        ]
        valid_df = transformed_df.select(final_columns)
        
        return valid_df
    ```
    Generate only the PySpark implementation code with no additional commentary or explanation.
    ## RESPONSE
    """,
)


def main_prompt(functions, input_tables, target_tables):
    return f"""
    Act as a senior PySpark architect. Generate a complete, production-ready main function that executes the provided transformation functions in the correct dimensional modeling order (dimensions first, facts last) with proper error handling and logging.

    ## TRANSFORMATION FUNCTIONS TO EXECUTE:
    {functions}
    ## INPUT TABLES:
    {input_tables}
    ## TARGET TABLES :
    {target_tables}

    ## REQUIREMENTS:
    1. Create a main function that:
        - Takes 'spark' as input parameter
        - Executes transformations in correct dependency order
        - Handles all tables in this sequence:
            * Dimension tables first (parent tables before children)
            * Fact tables last
        - For each table:
            * Logs start/end of processing
            * Reads source data ONCE and stores in variable
            * Applies transformation
            * Writes valid df into snowflake using write_to_snowflake function
            * NOT store the valid in the dimension_dfs 
            * Includes basic error handling
        - If a table depends on ANOTHER TABLE THAT HASN'T BEEN TRANSFORMED YET IN THE CURRENT RUN:
           * use read_from_snowflake to read those tables before transformation function

    2. Follow this exact pattern for each table (replace placeholders with actual values):
        try:
            logger.info("Starting processing for [TABLE_NAME]...")
            
            # Only read from Snowflake if needed for dependencies not yet processed
            if "[DEPENDENCY_TABLE]" not in dimension_dfs:
                dimension_dfs["[DEPENDENCY_TABLE]"] = read_from_snowflake(spark, "[DEPENDENCY_TABLE]")
            
            [source_df] = read_source_table(spark, "[SOURCE_TABLE]")
            valid_[table]_df = [TRANSFORM_FUNCTION]([source_df], dimension_dfs)
            write_to_snowflake(valid_[table]_df, "[TARGET_TABLE]")
            logger.info("Completed [TABLE_NAME] successfully")
        except Exception as e:
            logger.error("Failed processing [TABLE_NAME]: " + str(e))
            raise

    3. Key implementation details:
        - Read each source table ONLY ONCE at the beginning
        - Store source DataFrames in variables with meaningful names
        - NOT Store transformed DataFrames in the dimension_dfs dictionary 
        - Only read from Snowflake when absolutely necessary for dependencies
        - Prefer using in-memory transformed DataFrames over Snowflake reads

    4. Assume these exist:
        - spark (SparkSession)
        - read_source_table()
        - write_to_snowflake()
        - read_from_snowflake()
        - logger (configured Spark logger)
        - dimension_dfs = {{}} (empty dict at start)

    ## RESPONSE FORMAT:
    Return ONLY the Python code for the complete main function, with:
    - Proper function definition
    - Correct execution order based on table dependencies
    - Source tables read once and reused
    - Production-grade error handling
    - Informative logging
    - Clean, professional code style
    - Snowflake reads ONLY for missing dependencies

    Do not include any explanations or markdown formatting.
    ##RESPONSE :
    """
